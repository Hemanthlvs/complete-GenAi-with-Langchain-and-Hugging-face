# What is Groq and LPU?

*   **Groq.com** is a platform to access open-source models like Gemma 2, Llama 3 (70B & 8B parameters), and Mistral 8x7B.
*   These are **powerful open-source models from tech giants**.
*   **Groq** has deployed these models in its own cloud using an **LPU AI Inferencing Engine**.
*   **Why Groq?**
    *   It is an **AI infrastructure company** that delivers **fast AI inferencing**.
    *   The LLM space is competitive, with companies creating the best models (e.g., Llama 3, Gemma 2, GPT-4).
    *   **Inferencing speed** is crucial because LLM models are huge and deploying them on-premise for inferencing is costly.
*   **LPU (Language Processing Unit)**:
    *   A **hardware and software platform**.
    *   Delivers **exceptional compute speed, quality, and energy efficiency**.
    *   Results in **very fast responses**.
*   **Why LPU is faster than GPUs**:
    *   GPUs have **two LLM bottlenecks**: compute density and memory bandwidth.
    *   LPUs have **greater compute capacity** than GPUs and CPUs for LLMs.
    *   This **reduces time per word calculated**, generating text sequences much faster.
    *   Groq provides **APIs** to call these LLM models for applications.